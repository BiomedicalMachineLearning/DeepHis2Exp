{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f768582",
   "metadata": {},
   "source": [
    "# Benchmarking Data Augmentation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83d307",
   "metadata": {},
   "source": [
    "## ST-Net\n",
    "\n",
    "* Normalize the image\n",
    "* RandomRotation\n",
    "* VerticalFlip\n",
    "\n",
    "\n",
    "\"During training time, we augmented the dataset by randomly rotating the image by 0, 90, 180 or 270° and taking the mirror image 50% of the time. During the test time, we averaged the eight symmetries resulting from the rotations and reflections.\"\n",
    "\n",
    "Reference: https://github.com/bryanhe/ST-Net/blob/master/stnet/cmd/run_spatial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb70892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                  torchvision.transforms.RandomVerticalFlip(),\n",
    "                                  torchvision.transforms.RandomApply([torchvision.transforms.RandomRotation((90, 90))]),\n",
    "                                  torchvision.transforms.ToTensor(),\n",
    "                                  torchvision.transforms.Normalize(mean=mean, std=std)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcbeb2",
   "metadata": {},
   "source": [
    "## HistoGene\n",
    "\n",
    "* ColorJitter\n",
    "* RandomHorizontalFlip\n",
    "* RandomRotation(degrees=180)\n",
    "\n",
    "Reference: https://github.com/maxpmx/HisToGene/blob/main/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0644d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ColorJitter(0.5,0.5,0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(degrees=180),\n",
    "            transforms.ToTensor()\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bb63f",
   "metadata": {},
   "source": [
    "## Hist2ST\n",
    "* RandomGrayscale(0.1)\n",
    "* RandomRotation(90)\n",
    "* RandomHorizontalFlip(0.2)\n",
    "* Generate 5 additional augmented patches, using MLP to learn the parameters to fusion 6 patches.\n",
    "\n",
    "\"To alleviate the impact by the small spatial transcriptomics data, we used a similar self-distillation strategy to the previous study to learn the “dark knowledge” from augmented samples. Specifically, for each image patch (anchor image patch), we generate five augmented image patches through random grayscale, rotation, and horizontal flip. Each anchor image patch and its augmented images are fed into our model and six predicted gene expressions will be outputted. We set learnable parameters for each predicted gene expression from the augmented image patch to learn their contribution to the final predicted gene expression.\"\n",
    "\n",
    "Reference: https://github.com/biomed-AI/Hist2ST/blob/main/HIST2ST.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomGrayscale(0.1),\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomHorizontalFlip(0.2),\n",
    "])\n",
    "\n",
    "# Learnable parameters for each generated patch\n",
    "coef = nn.Sequential(\n",
    "    nn.Linear(dim,dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim,1),)\n",
    "\n",
    "# Data augmentation for each generated patch, save the \n",
    "def aug(patch,center,adj):\n",
    "    \"\"\"\n",
    "    Bake is the number of generated patches\n",
    "    coef is the parameter for each generated patch, coef = MLP(in_dim = feature, out_dim = 1)\n",
    "    \"\"\"\n",
    "    bake_x=[]\n",
    "    for i in range(bake):\n",
    "        new_patch = transform(patch.squeeze(0)).unsqueeze(0)\n",
    "        gene_exp,_,coef = model(new_patch, center, adj, aug=True)\n",
    "        bake_x.append((gene_exp.unsqueeze(0),coef.unsqueeze(0)))\n",
    "    return bake_x\n",
    "\n",
    "# Fuse generated and original gene expression.\n",
    "def distillation(bake_x):\n",
    "    # bake_x is a tuple, which include predicted gene expression and learnable coeficient\n",
    "    new_x,coef=zip(*bake_x)\n",
    "    coef=torch.cat(coef,0)\n",
    "    new_x=torch.cat(new_x,0)\n",
    "    coef=F.softmax(coef,dim=0)\n",
    "    new_x=(new_x*coef).sum(0)\n",
    "    return new_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165dced",
   "metadata": {},
   "source": [
    "## DeepSpace\n",
    "* Flip\n",
    "* Crop\n",
    "* Color\n",
    "* Random\n",
    "\n",
    "\"For image augmentation, we randomly applied image-transform functions of flipping (RandomRotate90, Flip, and Transpose), cropping (RandomResizedCrop), noise (IAAAdditiveGaussianNoise and GaussNoise), blurring (MotionBlur, MedianBlur, and Blur), distortion (OpticalDistortion, GridDistortion, IAAPiecewiseAffine, and ShiftScaleRotate), contrast (RandomContrast, RandomGamma, and RandomBrightness), and color-shifting (HueSaturationValue, ChannelShuffle, and RGBShift) in Albumentations library (version 0.4.5)32.\"\n",
    "\n",
    "Reference: https://github.com/tmonjo/DeepSpaCE/blob/main/BasicLib.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb1702f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2111b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 224\n",
      "mean: (0.485, 0.456, 0.406)\n",
      "std: (0.229, 0.224, 0.225)\n"
     ]
    }
   ],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "size = 224\n",
    "print(\"size: \"+str(size))\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "print(\"mean: \"+str(mean))\n",
    "\n",
    "std = (0.229, 0.224, 0.225)\n",
    "print(\"std: \"+str(std))\n",
    "\n",
    "ImageTransform = {\n",
    "    \n",
    "'flip': albu.Compose([\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "        albu.Flip(p=0.5),\n",
    "        albu.Transpose(p=0.5)\n",
    "    ], p=1.0),\n",
    "    \n",
    "'crop': albu.Compose([\n",
    "        albu.RandomResizedCrop(height=size, width=size, scale=(0.5, 1.0), p=0.5),\n",
    "    ], p=1.0),\n",
    "    \n",
    "'random': albu.Compose([\n",
    "            albu.OneOf([\n",
    "                albu.OneOf([\n",
    "                    albu.GaussNoise(p=1.0)\n",
    "                ], p=1.0),\n",
    "                albu.OneOf([\n",
    "                    albu.MotionBlur(p=1.0),\n",
    "                    albu.MedianBlur(p=1.0),\n",
    "                    albu.Blur(p=1.0)\n",
    "                ], p=1.0),\n",
    "                albu.OneOf([\n",
    "                    albu.OpticalDistortion(p=1.0),\n",
    "                    albu.GridDistortion(p=1.0),\n",
    "                    albu.ShiftScaleRotate(p=1.0)\n",
    "                ], p=1.0),\n",
    "            ], p=1.0),            \n",
    "        ], p=1.0),\n",
    "    \n",
    "'color': albu.Compose([\n",
    "            albu.HueSaturationValue(p=0.5),\n",
    "            albu.ChannelShuffle(p=0.5),\n",
    "            albu.RGBShift(p=0.5)\n",
    "            ], p=1.0),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27420d01",
   "metadata": {},
   "source": [
    "## BLEEP\n",
    "\n",
    "* HorizontalFlip and VerticalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05319ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def transform(image):\n",
    "    image = Image.fromarray(image)\n",
    "    # Random flipping and rotations\n",
    "    if random.random() > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "    if random.random() > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "    angle = random.choice([180, 90, 0, -90])\n",
    "    image = TF.rotate(image, angle)\n",
    "    return np.asarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02497a",
   "metadata": {},
   "source": [
    "## STimage\n",
    "\n",
    "\n",
    "\"In the STimage pipeline, we perform stain normalisation for each of all the images, such that the mean R, G and B channel intensities of the normalised images were similar to those of a template images, while preserving the original colour distribution patterns. STimage uses StainTool V2.1.3 to perform Vahadane [15] normalisation as the default option. In addition, the nature of the tissue sectioning, will inevitably eventuate in some tiles that contain a low tissue coverage. These tiles should be removed. STimage uses OpenCV2 for tissue masking and removes the tiles with tissue coverage lower than 70%.\"\n",
    "\n",
    "Reference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934a97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71b0898",
   "metadata": {},
   "source": [
    "## VICReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import ImageOps, ImageFilter\n",
    "\n",
    "def GaussianBlur(p):\n",
    "    if np.random.rand() < p:\n",
    "        sigma = np.random.rand() * 1.9 + 0.1\n",
    "        return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def Solarization(p):\n",
    "    if np.random.rand() < self.p:\n",
    "        return ImageOps.solarize(img)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def VICRegTransform():\n",
    "    transform1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    224, interpolation=InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1\n",
    "                        )\n",
    "                    ],\n",
    "                    p=0.8,\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=1.0),\n",
    "                Solarization(p=0.0),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    transform2 = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    224, interpolation=InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1\n",
    "                        )\n",
    "                    ],\n",
    "                    p=0.8,\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                GaussianBlur(p=0.1),\n",
    "                Solarization(p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f27d5",
   "metadata": {},
   "source": [
    "## Maske auto-encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c54996",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
