{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29fcc6c1-fe1d-4318-a8a1-9cc122f7aee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/imb/uqjxie6/benchmmarking/DeepHis2Exp/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd /scratch/imb/uqjxie6/benchmmarking/DeepHis2Exp/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf16315-b1db-4bfa-b916-b0c701521b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Union, Dict, Optional, Tuple, BinaryIO\n",
    "import h5py\n",
    "import json\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata\n",
    "from anndata import (\n",
    "    AnnData,\n",
    "    read_csv,\n",
    "    read_text,\n",
    "    read_excel,\n",
    "    read_mtx,\n",
    "    read_loom,\n",
    "    read_hdf,\n",
    ")\n",
    "from anndata import read as read_h5ad\n",
    "from anndata import read_h5ad\n",
    "import scanpy as sc\n",
    "from scanpy import read_visium, read_10x_mtx\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd \n",
    "import scprep as scp\n",
    "import anndata as ad\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "def read_visium_alex(\n",
    "    path: Union[str, Path],\n",
    "    genome: Optional[str] = None,\n",
    "    *,\n",
    "    count_dir: str = \"raw_feature_bc_matrix\",\n",
    "    library_id: str = None,\n",
    "    load_images: Optional[bool] = True,\n",
    "    source_image_path: Optional[Union[str, Path]] = None,\n",
    ") -> AnnData:\n",
    "    path = Path(path)\n",
    "    adata = read_10x_mtx(path / count_dir)\n",
    "\n",
    "    adata.uns[\"spatial\"] = dict()\n",
    "\n",
    "    from h5py import File\n",
    "\n",
    "    adata.uns[\"spatial\"][library_id] = dict()\n",
    "\n",
    "    if load_images:\n",
    "        files = dict(\n",
    "            tissue_positions_file=path / 'spatial/tissue_positions_list.csv',\n",
    "            scalefactors_json_file=path / 'spatial/scalefactors_json.json',\n",
    "            hires_image=path / 'spatial/tissue_hires_image.png',\n",
    "            lowres_image=path / 'spatial/tissue_lowres_image.png',\n",
    "        )\n",
    "\n",
    "        # check if files exists, continue if images are missing\n",
    "        for f in files.values():\n",
    "            if not f.exists():\n",
    "                if any(x in str(f) for x in [\"hires_image\", \"lowres_image\"]):\n",
    "                    logg.warning(\n",
    "                        f\"You seem to be missing an image file.\\n\"\n",
    "                        f\"Could not find '{f}'.\"\n",
    "                    )\n",
    "                else:\n",
    "                    raise OSError(f\"Could not find '{f}'\")\n",
    "\n",
    "        adata.uns[\"spatial\"][library_id]['images'] = dict()\n",
    "        for res in ['hires', 'lowres']:\n",
    "            try:\n",
    "                adata.uns[\"spatial\"][library_id]['images'][res] = imread(\n",
    "                    str(files[f'{res}_image'])\n",
    "                )\n",
    "            except Exception:\n",
    "                raise OSError(f\"Could not find '{res}_image'\")\n",
    "\n",
    "        # read json scalefactors\n",
    "        adata.uns[\"spatial\"][library_id]['scalefactors'] = json.loads(\n",
    "            files['scalefactors_json_file'].read_bytes()\n",
    "        )\n",
    "\n",
    "        adata.uns[\"spatial\"][library_id][\"metadata\"] = {}\n",
    "\n",
    "        # read coordinates\n",
    "        positions = pd.read_csv(files['tissue_positions_file'], header=None)\n",
    "        positions.columns = [\n",
    "            'barcode',\n",
    "            'in_tissue',\n",
    "            'array_row',\n",
    "            'array_col',\n",
    "            'pxl_col_in_fullres',\n",
    "            'pxl_row_in_fullres',\n",
    "        ]\n",
    "        positions.index = positions['barcode']\n",
    "\n",
    "        adata.obs = adata.obs.join(positions, how=\"left\")\n",
    "\n",
    "        adata.obsm['spatial'] = adata.obs[\n",
    "            ['pxl_row_in_fullres', 'pxl_col_in_fullres']\n",
    "        ].to_numpy()\n",
    "        adata.obs.drop(\n",
    "            columns=['barcode', 'pxl_row_in_fullres', 'pxl_col_in_fullres'],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # put image path in uns\n",
    "        if source_image_path is not None:\n",
    "            # get an absolute path\n",
    "            source_image_path = str(Path(source_image_path).resolve())\n",
    "            adata.uns[\"spatial\"][library_id][\"metadata\"][\"source_image_path\"] = str(\n",
    "                source_image_path\n",
    "            )\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473b1732-55ad-40a4-8d47-74563d23379f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir1 = \"../data/Alex_NatGen/\"\n",
    "data_dir2 = \"../data/Breast_Cancer_10x/\"\n",
    "\n",
    "samps1 = [\"1142243F\", \"CID4290\", \"CID4465\", \"CID44971\", \"CID4535\"]\n",
    "samps2 = [\"block1\", \"block2\", \"FFPE\", \"CytAssist_FFPE\"]\n",
    "\n",
    "samples1 = {i:data_dir1 + i for i in samps1}\n",
    "samples2 = {i:data_dir2 + i for i in samps2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e979c3-6404-4bee-ac96-f50dab45b2ca",
   "metadata": {},
   "source": [
    "You may need to rename the files in the two directories to match the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48ff837b-5296-40f8-8c21-ac61fcf7b6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;35m1142243F.tif\u001b[0m            metadata.csv            \u001b[01;34mspatial\u001b[0m/\n",
      "\u001b[01;34mfiltered_count_matrix\u001b[0m/  \u001b[01;34mraw_feature_bc_matrix\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls {data_dir1 + samps1[0]} #  For Alex_NatGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21200870-3efe-4633-8179-3d753a350902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mfiltered_feature_bc_matrix\u001b[0m/  filtered_feature_bc_matrix.h5  \u001b[01;35mimage.tif\u001b[0m  \u001b[01;34mspatial\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls {data_dir2 + samps2[0]} # For Breast_Cancer_10x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345e4ed4-cc7a-4a12-ba1f-00ead9fbde19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 1.36 s, total: 1min 27s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adata_dict1 = {name: read_visium_alex(path, library_id = name, source_image_path = path + f\"/{name}.tif\") for name,path in samples1.items()}\n",
    "# adata_dict2 = {name: read_visium(path, library_id = name, source_image_path = path + \"/image.tif\") for name,path in samples2.items()}\n",
    "\n",
    "adata_dict = {**adata_dict1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cde2d7b-7403-49a6-9f6d-a6044f9fbc45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/imb/uqjxie6/software/stimage5/lib/python3.8/site-packages/anndata/_core/anndata.py:1828: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 24960 × 36601\n",
       "    obs: 'in_tissue', 'array_row', 'array_col'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adatas = [v for k,v in adata_dict.items()]\n",
    "adatas\n",
    "anndata.concat(adatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8cef27-e247-4e62-ba42-f11bfd709ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../models/Hist2ST/\")\n",
    "from utils import read_tiff, get_data\n",
    "from graph_construction import calcADJ\n",
    "from collections import defaultdict as dfd\n",
    "\n",
    "def read_tiff(path):\n",
    "    Image.MAX_IMAGE_PIXELS = None\n",
    "    im = Image.open(path)\n",
    "    imarray = np.array(im)\n",
    "    # I = plt.imread(path)\n",
    "    return im\n",
    "\n",
    "class ViT_Anndata(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about ViT_SKIN\"\"\"\n",
    "    def __init__(self, adata_dict, train_set, gene_list, train=True,r=4,norm=False,flatten=True,ori=True,adj=True,prune='NA',neighs=4):\n",
    "        super(ViT_Anndata, self).__init__()\n",
    "\n",
    "        self.r = 224//r\n",
    "\n",
    "        names = list(adata_dict.keys())\n",
    "\n",
    "        self.ori = ori\n",
    "        self.adj = adj\n",
    "        self.norm = norm\n",
    "        self.train = train\n",
    "        self.flatten = flatten\n",
    "        self.gene_list = gene_list\n",
    "        samples = names\n",
    "        tr_names = train_set\n",
    "        te_names = list(set(samples)-set(tr_names))\n",
    "\n",
    "        if train:\n",
    "            self.names = tr_names\n",
    "        else:\n",
    "            self.names = te_names\n",
    "\n",
    "        print(\"Eval set: \", te_names)\n",
    "        \n",
    "        self.adata_dict = {k: v for k, v in adata_dict.items() if k in self.names}\n",
    "    \n",
    "        print('Loading imgs...')\n",
    "        self.img_dict = {i:torch.Tensor(np.array(self.get_img(i))) for i in self.names}\n",
    "\n",
    "        self.gene_set = list(gene_list)\n",
    "        if self.norm:\n",
    "            self.exp_dict = {\n",
    "                i:sc.pp.scale(scp.transform.log(scp.normalize.library_size_normalize(m.to_df()[self.gene_set].values))).astype(np.float64)\n",
    "                for i,m in self.adata_dict.items()\n",
    "            }\n",
    "        else:\n",
    "            self.exp_dict = {\n",
    "                i:scp.transform.log(scp.normalize.library_size_normalize(m.to_df()[self.gene_set].values)).astype(np.float64) \n",
    "                for i,m in self.adata_dict.items()\n",
    "            }\n",
    "        if self.ori:\n",
    "            self.ori_dict = {i:m.to_df()[self.gene_set].values.astype(np.float64) for i,m in self.adata_dict.items()}\n",
    "            self.counts_dict={}\n",
    "            for i,m in self.ori_dict.items():\n",
    "                n_counts=m.sum(1)\n",
    "                sf = n_counts / np.median(n_counts)\n",
    "                self.counts_dict[i]=sf.astype(np.float64)\n",
    "        self.center_dict = {\n",
    "            i:np.floor(m.obsm[\"spatial\"]).astype(int)\n",
    "            for i,m in self.adata_dict.items()\n",
    "        }\n",
    "        self.loc_dict = {i:m.obs[['array_col', 'array_row']].values for i,m in self.adata_dict.items()}\n",
    "        self.adj_dict = {\n",
    "            i:calcADJ(m,neighs,pruneTag=prune)\n",
    "            for i,m in self.loc_dict.items()\n",
    "        }\n",
    "        self.patch_dict=dfd(lambda :None)\n",
    "        self.lengths = [i.n_obs for i in self.adata_dict.values()]\n",
    "        self.cumlen = np.cumsum(self.lengths)\n",
    "        self.id2name = dict(enumerate(self.names))\n",
    "\n",
    "\n",
    "    def filter_helper(self):\n",
    "        a = np.zeros(len(self.gene_list))\n",
    "        n = 0\n",
    "        for i,exp in self.exp_dict.items():\n",
    "            n += exp.shape[0]\n",
    "            exp[exp>0] = 1\n",
    "            for j in range((len(self.gene_list))):\n",
    "                a[j] += np.sum(exp[:,j])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID=self.id2name[index]\n",
    "        im = self.img_dict[ID].permute(1,0,2)\n",
    "\n",
    "        exps = self.exp_dict[ID]\n",
    "        if self.ori:\n",
    "            oris = self.ori_dict[ID]\n",
    "            sfs = self.counts_dict[ID]\n",
    "        adj=self.adj_dict[ID]\n",
    "        centers = self.center_dict[ID]\n",
    "        loc = self.loc_dict[ID]\n",
    "        patches = self.patch_dict[ID]\n",
    "        positions = torch.LongTensor(loc)\n",
    "        patch_dim = 3 * self.r * self.r * 4\n",
    "        exps = torch.Tensor(exps)\n",
    "        if patches is None:\n",
    "            n_patches = len(centers)\n",
    "            if self.flatten:\n",
    "                patches = torch.zeros((n_patches,patch_dim))\n",
    "            else:\n",
    "                patches = torch.zeros((n_patches,3,2*self.r,2*self.r))\n",
    "\n",
    "            for i in range(n_patches):\n",
    "                center = centers[i]\n",
    "                x, y = center\n",
    "                patch = im[(x-self.r):(x+self.r),(y-self.r):(y+self.r),:]\n",
    "                if self.flatten:\n",
    "                    patches[i] = patch.flatten()\n",
    "                else:\n",
    "                    patches[i]=patch.permute(2,0,1)\n",
    "            self.patch_dict[ID]=patches\n",
    "        data=[patches, positions, exps]\n",
    "        if self.adj:\n",
    "            data.append(adj)\n",
    "        if self.ori:\n",
    "            data+=[torch.Tensor(oris),torch.Tensor(sfs)]\n",
    "        data.append(torch.Tensor(centers))\n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.exp_dict)\n",
    "\n",
    "    def get_img(self,name):\n",
    "        path = self.adata_dict[name].uns[\"spatial\"][name][\"metadata\"][\"source_image_path\"]\n",
    "        im = read_tiff(path)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372973a0-12a2-4a4a-b722-622d44f5d317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "# sys.path.append(\"../../scripts/\")\n",
    "\n",
    "import time\n",
    "\n",
    "from window_adata import *\n",
    "from read_stimage_genes import *\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as tf\n",
    "from tqdm import tqdm\n",
    "from predict import *\n",
    "from HIST2ST import *\n",
    "from dataset import ViT_HER2ST, ViT_SKIN\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from copy import deepcopy as dcp\n",
    "import pickle\n",
    "from collections import defaultdict as dfd\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Union, Dict, Optional, Tuple, BinaryIO\n",
    "import h5py\n",
    "import json\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata\n",
    "from anndata import (\n",
    "    AnnData,\n",
    "    read_csv,\n",
    "    read_text,\n",
    "    read_excel,\n",
    "    read_mtx,\n",
    "    read_loom,\n",
    "    read_hdf,\n",
    ")\n",
    "from anndata import read as read_h5ad\n",
    "from anndata import read_h5ad\n",
    "import scanpy as sc\n",
    "from scanpy import read_visium, read_10x_mtx\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd \n",
    "import scprep as scp\n",
    "import anndata as ad\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "\n",
    "\n",
    "def calculate_correlation(attr_1, attr_2):\n",
    "    r = pearsonr(attr_1, \n",
    "                       attr_2)[0]\n",
    "    return r\n",
    "\n",
    "\n",
    "data_dir1 = \"../../data/pfizer/\"\n",
    "\n",
    "samps1 = [\"VLP79_D\",\"VLP82_A\",\"VLP79_A\",\"VLP80_D\",\"VLP83_A\",\"VLP80_A\",\"VLP81_A\",\"VLP82_D\",\"VLP83_D\",\"VLP78_A\"]\n",
    "\n",
    "samples1 = {i:data_dir1 + i for i in samps1}\n",
    "\n",
    "adata_dict1 = {name: read_visium(path, library_id = name, source_image_path = path + f\"/image.tif\") for name,path in samples1.items()}\n",
    "\n",
    "adata_dict0 = {**adata_dict1}\n",
    "\n",
    "for k,v in adata_dict0.items():\n",
    "    v.obsm[\"spatial\"] = v.obsm[\"spatial\"].astype(np.int64)\n",
    "    v.obs[['in_tissue','array_row','array_col']] = v.obs[['in_tissue','array_row','array_col']].astype(np.int64)\n",
    "\n",
    "\n",
    "sizes = [3000 for i in range(len(adata_dict0))]\n",
    "\n",
    "adata_dict = window_adata(adata_dict0, sizes)\n",
    "\n",
    "# gene_list = read_gene_set(\"../../data/pfizer/\") # all_adata.h5ad\n",
    "gene_list = ['CD4', 'TRAC', 'CXCR4']\n",
    "gene_list = set(gene_list)\n",
    "\n",
    "gene_list = intersect_section_genes(gene_list, adata_dict)\n",
    "n_genes = len(gene_list)\n",
    "\n",
    "train_set = list(set(list(adata_dict.keys())) - set([i for i in list(adata_dict.keys()) if 'VLP78_A' in i]))\n",
    "\n",
    "\n",
    "from data_vit import ViT_Anndata\n",
    "\n",
    "\n",
    "device='cuda'\n",
    "tag='5-7-2-8-4-16-32'\n",
    "k,p,d1,d2,d3,h,c=map(lambda x:int(x),tag.split('-'))\n",
    "dropout=0.2\n",
    "random.seed(12000)\n",
    "np.random.seed(12000)\n",
    "torch.manual_seed(12000)\n",
    "torch.cuda.manual_seed(12000)\n",
    "torch.cuda.manual_seed_all(12000)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "df = pd.DataFrame()\n",
    "i = int(sys.argv[1])\n",
    "# i=1\n",
    "fold = i\n",
    "# test_sample = names[fold]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gene_list = [\"COX6C\",\"TTLL12\", \"PABPC1\", \"GNAS\", \"HSP90AB1\", \n",
    "#            \"TFF3\", \"ATP1A1\", \"B2M\", \"FASN\", \"SPARC\", \"CD74\", \"CD63\", \"CD24\", \"CD81\"]\n",
    "# genes = len(gene_list)\n",
    "\n",
    "\n",
    "trainset = ViT_Anndata(adata_dict = adata_dict, train_set = train_set, gene_list = gene_list,\n",
    "            train=True,flatten=False,adj=True,ori=True,prune='NA',neighs=4, \n",
    "        )\n",
    "\n",
    "print(\"LOADED TRAINSET\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=1, num_workers=0, shuffle=True)\n",
    "\n",
    "model=Hist2ST(\n",
    "    depth1=d1, depth2=d2,depth3=d3,n_genes=n_genes,\n",
    "    kernel_size=k, patch_size=p,\n",
    "    heads=h, channel=c, dropout=0.2,\n",
    "    zinb=0.25, nb=False,\n",
    "    bake=5, lamb=0.5, n_pos=128,\n",
    ")\n",
    "\n",
    "logger=None\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[0], max_epochs=1,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "start_train = time.perf_counter()\n",
    "trainer.fit(model, train_loader)\n",
    "\n",
    "end_train = time.perf_counter()\n",
    "import os\n",
    "if not os.path.isdir(\"../../trained_models/\"):\n",
    "    os.mkdir(\"../../trained_models/\")\n",
    "\n",
    "torch.save(model.state_dict(),f\"../trained_models/{fold}-Hist2ST.ckpt\")\n",
    "\n",
    "\n",
    "def evall(test_sample, gene_list):\n",
    "    \n",
    "    train_set = list(set(list(adata_dict.keys())) - set([test_sample]))\n",
    "    testset = ViT_Anndata(adata_dict = adata_dict, train_set = train_set, gene_list = gene_list,\n",
    "                train=False,flatten=False,adj=True,ori=True,prune='NA',neighs=4, \n",
    "            )\n",
    "    test_loader = DataLoader(testset, batch_size=1, num_workers=0, shuffle=False)\n",
    "\n",
    "    adata_pred, adata_truth = test(model, test_loader,'cuda')\n",
    "\n",
    "    adata_pred.var_names = gene_list\n",
    "    adata_truth.var_names = gene_list\n",
    "\n",
    "    pred_adata = adata_pred.copy()\n",
    "    test_dataset = adata_truth.copy()\n",
    "    \n",
    "    test_sample = ','.join(list(test_sample))\n",
    "    \n",
    "    with open(f\"../../results/pf/hist2st_preds_{test_sample}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump([pred_adata,test_dataset], f)\n",
    "\n",
    "    for gene in pred_adata.var_names:\n",
    "        cor_val = calculate_correlation(pred_adata.to_df().loc[:,gene], test_dataset.to_df().loc[:,gene])\n",
    "        df = df.append(pd.Series([gene, cor_val, test_sample, \"Hist2ST\"], \n",
    "                             index=[\"Gene\", \"Pearson correlation\", \"Slide\", \"Method\"]),\n",
    "                  ignore_index=True)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    df.to_csv(\"../../results/pf/hist2st_cor_{}_{i}.csv\".format(test_sample, i))\n",
    "\n",
    "    with open(\"../../results/pf/hist2st_times.txt\", 'a') as f:\n",
    "        f.write(f\"{i} {test_sample} {end_train - start_train} - {time.strftime('%H:%M:%S', time.localtime())}\")\n",
    "\n",
    "\n",
    "\n",
    "evall([i for i in list(adata_dict.keys()) if 'VLP78_A' in i], gene_list)\n",
    "\n",
    "# gene_list = [\"COX6C\",\"TTLL12\", \"HSP90AB1\", \n",
    "#            \"TFF3\", \"ATP1A1\", \"B2M\", \"FASN\", \"SPARC\", \"CD74\", \"CD63\", \"CD24\", \"CD81\"]\n",
    "# evall(\"CytAssist_FFPE\", gene_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872017c9-8c8f-47d5-a25d-2f9c0fb4cb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval set:  ['CytAssist_FFPE', 'FFPE']\n",
      "Loading imgs...\n",
      "CPU times: user 1min 13s, sys: 32.2 s, total: 1min 46s\n",
      "Wall time: 52.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gene_list = [\"COX6C\",\"TTLL12\", \"PABPC1\", \"GNAS\", \"HSP90AB1\", \n",
    "           \"TFF3\", \"ATP1A1\", \"B2M\", \"FASN\", \"SPARC\", \"CD74\", \"CD63\", \"CD24\", \"CD81\"]\n",
    "train_set = list(set(list(adata_dict.keys())) - set([\"FFPE\", \"CytAssist_FFPE\"]))\n",
    "ds = ViT_Anndata(adata_dict = adata_dict, train_set = train_set, gene_list = gene_list,\n",
    "            train=True,flatten=False,adj=True,ori=True,prune='NA'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59185a7c-d568-4ec9-aa34-69254df86183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(ds, batch_size=1, num_workers=0, shuffle=True)\n",
    "iterator=iter(train_loader)\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(i)\n",
    "#     next(iterator)\n",
    "#     print(ds.patch_dict.keys())\n",
    "#     # print(ds.patch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92bb5b00-27ca-4de5-b6e7-361d5fdda31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  6\n"
     ]
    }
   ],
   "source": [
    "test = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecd44e99-6900-4836-ae56-617f54d734e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "619f9d46-0696-4e18-88b2-e110c9610a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4992, 14)\n",
      "(4992, 14)\n",
      "(4992, 14)\n",
      "(4992, 14)\n",
      "(4992, 14)\n",
      "(3798, 14)\n",
      "(3987, 14)\n"
     ]
    }
   ],
   "source": [
    "for i,m in ds.exp_dict.items():\n",
    "    print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad13d801-66f4-4b68-8629-90d15af57924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Index:  1\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "1\n",
      "Index:  2\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "2\n",
      "Index:  5\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "3\n",
      "Index:  3\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "4\n",
      "Index:  0\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "5\n",
      "Index:  4\n",
      "[torch.Size([1, 4992, 3, 112, 112]), torch.Size([1, 4992, 2]), torch.Size([1, 4992, 14]), torch.Size([1, 4992, 4992]), torch.Size([1, 4992, 14]), torch.Size([1, 4992]), torch.Size([1, 4992, 2])]\n",
      "6\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m([i\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test])\n",
      "File \u001b[0;32m/scratch/imb/uqjxie6/software/stimage5/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/imb/uqjxie6/software/stimage5/lib/python3.8/site-packages/torch/utils/data/dataloader.py:670\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m/scratch/imb/uqjxie6/software/stimage5/lib/python3.8/site-packages/torch/utils/data/dataloader.py:618\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    next(iterator)\n",
    "    print([i.shape for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ecba8a2-81ae-4e3e-bf70-99470bd64b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.img_dict['1142243F'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e25bc4f-b008-42d8-a713-cd6f306e8296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.patch_dict['CID44971'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e7cb283-1f6a-455d-af2d-f570d021133d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.center_dict['CID44971'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75c20d20-6bbb-4336-b330-0d76a939598d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16,   0],\n",
       "       [102,  50],\n",
       "       [ 43,   3],\n",
       "       ...,\n",
       "       [ 27,  45],\n",
       "       [ 41,  73],\n",
       "       [ 51,   7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.loc_dict['CID44971']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5f1204a-6765-4f96-851e-1ab54c124810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.adj_dict['CID44971'].count_nonzero() / ds.adj_dict['CID44971'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c738598d-bd31-41ef-83d9-872ce9b7c557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16, 102,  43,  ...,  27,  41,  51]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2c08d-1325-4d78-80da-538ea1e7897e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
