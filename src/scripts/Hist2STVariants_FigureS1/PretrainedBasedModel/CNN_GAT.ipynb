{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd60f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pytorch_lightning as pl\n",
    "import scanpy as sc\n",
    "import scprep as scp\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict as dfd\n",
    "from copy import deepcopy as dcp\n",
    "from pathlib import Path, PurePath\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile, Image\n",
    "from matplotlib.image import imread\n",
    "from scanpy import read_10x_mtx, read_visium\n",
    "from torch.utils.data import DataLoader\n",
    "from window_adata import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cace1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowing 1142243F\n",
      "Num spots:  4784\n",
      "246\n",
      "216\n",
      "222\n",
      "185\n",
      "77\n",
      "247\n",
      "246\n",
      "255\n",
      "247\n",
      "93\n",
      "246\n",
      "247\n",
      "255\n",
      "245\n",
      "94\n",
      "247\n",
      "246\n",
      "255\n",
      "238\n",
      "88\n",
      "130\n",
      "135\n",
      "132\n",
      "140\n",
      "55\n",
      "Total:  4787\n",
      "Windowing CID4290\n",
      "Num spots:  2714\n",
      "793\n",
      "576\n",
      "1001\n",
      "344\n",
      "Total:  2714\n",
      "Windowing CID4465\n",
      "Num spots:  1310\n",
      "345\n",
      "258\n",
      "149\n",
      "558\n",
      "Total:  1310\n",
      "Windowing CID44971\n",
      "Num spots:  1322\n",
      "491\n",
      "462\n",
      "339\n",
      "30\n",
      "Total:  1322\n",
      "Windowing CID4535\n",
      "Num spots:  1431\n",
      "564\n",
      "232\n",
      "632\n",
      "3\n",
      "Total:  1431\n",
      "Windowing 1160920F\n",
      "Num spots:  4895\n",
      "210\n",
      "251\n",
      "251\n",
      "239\n",
      "83\n",
      "226\n",
      "255\n",
      "232\n",
      "240\n",
      "102\n",
      "231\n",
      "230\n",
      "246\n",
      "247\n",
      "99\n",
      "238\n",
      "246\n",
      "247\n",
      "255\n",
      "93\n",
      "144\n",
      "147\n",
      "164\n",
      "160\n",
      "60\n",
      "Total:  4896\n",
      "Windowing block1\n",
      "Num spots:  3798\n",
      "139\n",
      "205\n",
      "219\n",
      "185\n",
      "10\n",
      "169\n",
      "246\n",
      "247\n",
      "255\n",
      "16\n",
      "189\n",
      "230\n",
      "205\n",
      "233\n",
      "0\n",
      "197\n",
      "156\n",
      "241\n",
      "228\n",
      "0\n",
      "72\n",
      "106\n",
      "129\n",
      "124\n",
      "0\n",
      "Total:  3801\n",
      "Windowing block2\n",
      "Num spots:  3987\n",
      "224\n",
      "247\n",
      "246\n",
      "229\n",
      "208\n",
      "246\n",
      "247\n",
      "231\n",
      "243\n",
      "207\n",
      "211\n",
      "196\n",
      "221\n",
      "195\n",
      "254\n",
      "205\n",
      "81\n",
      "97\n",
      "108\n",
      "92\n",
      "Total:  3988\n",
      "Windowing FFPE\n",
      "Num spots:  2518\n",
      "50\n",
      "190\n",
      "188\n",
      "79\n",
      "0\n",
      "169\n",
      "219\n",
      "216\n",
      "189\n",
      "0\n",
      "182\n",
      "215\n",
      "201\n",
      "192\n",
      "0\n",
      "68\n",
      "138\n",
      "159\n",
      "63\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "Total:  2519\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Integrate two visium datasets \"\"\"\n",
    "data_dir1 = \"./Alex_NatGen_6BreastCancer/\"\n",
    "data_dir2 = \"./breast_cancer_10x_visium/\"\n",
    "\n",
    "samps1 = [\"1142243F\", \"CID4290\", \"CID4465\", \"CID44971\", \"CID4535\", \"1160920F\"]\n",
    "samps2 = [\"block1\", \"block2\", \"FFPE\"]\n",
    "\n",
    "sampsall = samps1 + samps2\n",
    "samples1 = {i:data_dir1 + i for i in samps1}\n",
    "samples2 = {i:data_dir2 + i for i in samps2}\n",
    "\n",
    "# Marker gene list\n",
    "gene_list = [\"COX6C\",\"TTLL12\", \"HSP90AB1\", \"TFF3\", \"ATP1A1\", \"B2M\", \"FASN\", \"SPARC\", \"CD74\", \"CD63\", \"CD24\", \"CD81\"]\n",
    "\n",
    "# # Load windowed dataset\n",
    "import pickle\n",
    "with open('10x_visium_dataset_without_window.pickle', 'rb') as f:\n",
    "    adata_dict0 = pickle.load(f)\n",
    "    \n",
    "# Define the gridding size\n",
    "sizes = [4000 for i in range(len(adata_dict0))]\n",
    "\n",
    "# Split tiles into smaller patches according to gridding size\n",
    "adata_dict = window_adata(adata_dict0, sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e174f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "from data_vit import ViT_Anndata\n",
    "\n",
    "def dataset_wrap(fold = 0, train=True, dataloader=True):\n",
    "    test_sample = sampsall[fold]\n",
    "    test_sample_orig = sampsall[fold] # Split one sample as test sample\n",
    "    val_sample = list(set(sampsall)-set(sampsall[fold]))[:3] # Split 3 samples as validation samples\n",
    "    train_sample = list(set(sampsall)-set(test_sample)-set(val_sample)) # Other samples are training samples\n",
    "\n",
    "    tr_name = list(set([i for i in list(adata_dict.keys()) for tr in train_sample if tr in i]))\n",
    "    val_name = list(set([i for i in list(adata_dict.keys()) for val in val_sample if val in i]))\n",
    "#     te_name = list(set([i for i in list(adata_dict.keys()) if test_sample in i]))\n",
    "    te_name = test_sample\n",
    "    \n",
    "    if train:\n",
    "        print(\"LOADED TRAINSET\")\n",
    "        trainset = ViT_Anndata(adata_dict = adata_dict, train_set = tr_name, gene_list = gene_list, train=True, flatten=False, ori=True, prune='NA', neighs=4, )\n",
    "        valset = ViT_Anndata(adata_dict = adata_dict, train_set = val_name, gene_list = gene_list, train=True, flatten=False, ori=True, prune='NA', neighs=4, )\n",
    "        train_loader = DataLoader(trainset, batch_size=1, num_workers=0, shuffle=True)\n",
    "        val_loader = DataLoader(valset, batch_size=1, num_workers=0, shuffle=False)\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    else:\n",
    "        print(\"LOADED TESTSET\")\n",
    "        print(\"Test sample\", te_name)\n",
    "        testset = ViT_Anndata(adata_dict = adata_dict0, train_set = [te_name], gene_list = gene_list, train=False, flatten=False, ori=True, prune='NA', neighs=4, )\n",
    "        test_loader = DataLoader(testset, batch_size=1, num_workers=0, shuffle=False)\n",
    "        return test_loader\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50fb59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold = 0\n",
    "# name = \"VICReg\"\n",
    "# train_loader, val_loader = dataset_wrap(fold = fold, train=True, dataloader= True)\n",
    "# test_loader = dataset_wrap(fold = fold, train=False, dataloader= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d43931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[easydl] tensorflow not available!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "from gcn import *\n",
    "from transformer import *\n",
    "from NB_module import *\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy as dcp\n",
    "from collections import defaultdict as dfd\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# set random seed\n",
    "def setup_seed(seed=12000):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def get_R(data1,data2,dim=1,func=pearsonr):\n",
    "    adata1=data1.X\n",
    "    adata2=data2.X\n",
    "    r1,p1=[],[]\n",
    "    for g in range(data1.shape[dim]):\n",
    "        if dim==1:\n",
    "            r,pv=func(adata1[:,g],adata2[:,g])\n",
    "        elif dim==0:\n",
    "            r,pv=func(adata1[g,:],adata2[g,:])\n",
    "        r1.append(r)\n",
    "        p1.append(pv)\n",
    "    r1=np.array(r1)\n",
    "    p1=np.array(p1)\n",
    "    return r1,p1\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, num_heads=2, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in: Dimensionality of input features\n",
    "            c_out: Dimensionality of output features\n",
    "            num_heads: Number of heads, i.e. attention mechanisms to apply in parallel. The\n",
    "                        output features are equally split up over the heads if concat_heads=True.\n",
    "            concat_heads: If True, the output of the different heads is concatenated instead of averaged.\n",
    "            alpha: Negative slope of the LeakyReLU activation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modules and parameters needed in the layer\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out))  # One per head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_feats: Input features of the node. Shape: [batch_size, c_in]\n",
    "            adj_matrix: Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs: If True, the attention weights are printed during the forward pass\n",
    "                               (for debugging purposes)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        # Apply linear layer and sort nodes by head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # We need to calculate the attention logits for every edge in the adjacency matrix\n",
    "        # Doing this on all possible combinations of nodes is very expensive\n",
    "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
    "        # Returns indices where the adjacency matrix is not 0 => edges\n",
    "        edges = adj_matrix.nonzero(as_tuple=False)\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat(\n",
    "            [\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "                torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # Index select returns a tensor with node_feats_flat being indexed at the desired positions\n",
    "\n",
    "        # Calculate attention MLP output (independent for each head)\n",
    "        attn_logits = torch.einsum(\"bhc,hc->bh\", a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Map list of attention values back into a matrix\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape + (self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[..., None].repeat(1, 1, 1, self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Weighted average of attention\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        atten = attn_probs.permute(0, 3, 1, 2)\n",
    "        node_feats = torch.einsum(\"bijh,bjhc->bihc\", attn_probs, node_feats)\n",
    "\n",
    "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "        return node_feats, atten\n",
    "\n",
    "class Feature_extractor(nn.Module):\n",
    "    def __init__(self, name=\"resnet\", dim=1024, num_layer=2, ):\n",
    "        super().__init__()\n",
    "        self.ft_extra = self.ft_extr(name)\n",
    "        self.emb_dim = self.return_emb(name)\n",
    "        self.projection = nn.Sequential(nn.Linear(self.emb_dim, dim))\n",
    "        self.GATLayer=GATLayer(c_in=dim, c_out=dim)\n",
    "        self.GAT=nn.ModuleList([self.GATLayer for _ in range(num_layer)])\n",
    "        self.jknet=nn.Sequential(\n",
    "            nn.LSTM(dim,dim,2),\n",
    "            SelectItem(0),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,patch,adj):\n",
    "        # Resize the tiles\n",
    "\n",
    "        x = transforms.Resize(224)(patch)\n",
    "        x = self.ft_extra(x)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x).unsqueeze(0)\n",
    "        \n",
    "        # GAT with layer-aggregation\n",
    "        jk=[]\n",
    "        for layer in self.GAT:\n",
    "            x, attn=layer(x,adj)\n",
    "            jk.append(x)\n",
    "        x=torch.cat(jk,0)\n",
    "        \n",
    "        # Jumping knowledge-LSTM\n",
    "        x=self.jknet(x).mean(0)\n",
    "        return x, attn\n",
    "    \n",
    "    # Define a function to fine-tune a pretrained model\n",
    "    def ft_extr(self, name):\n",
    "        if name == \"resnet\":\n",
    "            model_ft = torchvision.models.resnet50(weights=models.ResNet50_Weights)\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False\n",
    "            model_ft.fc = nn.Sequential(nn.Identity())\n",
    "            dim = 2048\n",
    "        elif name == \"efficient\":\n",
    "            model_ft = torchvision.models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights)\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False\n",
    "            model_ft.classifier = nn.Sequential(nn.Identity())\n",
    "            dim = 1280\n",
    "        elif name == 'swin':\n",
    "            model_ft = torchvision.models.swin_s(weights=models.Swin_S_Weights)\n",
    "            for param in model_ft.parameters():\n",
    "                param.requires_grad = False\n",
    "            model_ft.head = nn.Sequential(nn.Identity())\n",
    "            dim = 768\n",
    "        return model_ft\n",
    "    \n",
    "    def return_emb(self, name):\n",
    "        dim=2048\n",
    "        if name==\"resnet\":\n",
    "            dim=2048\n",
    "        elif name==\"efficient\":\n",
    "            dim=1280\n",
    "        elif name==\"swin\":\n",
    "            dim=768\n",
    "        return dim\n",
    "\n",
    "class CNN_GAT(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4, name=\"resnet\", dim=1024, n_genes=12, num_layer=4, \n",
    "                 zinb=0.25, nb=False, policy='mean', bake=5, lamb=0.5, \n",
    "                ):\n",
    "        super().__init__()\n",
    "#          self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nb=nb\n",
    "        self.zinb=zinb\n",
    "        self.bake=bake\n",
    "        self.lamb=lamb\n",
    "        \n",
    "        \n",
    "        \"\"\" Feature Extractor \"\"\"\n",
    "        self.vit = Feature_extractor(name=name, num_layer=num_layer)\n",
    "        \n",
    "        self.n_genes=n_genes\n",
    "        \"\"\" ZINB Loss \"\"\"\n",
    "        if self.zinb>0:\n",
    "            if self.nb:\n",
    "                self.hr=nn.Linear(dim, n_genes)\n",
    "                self.hp=nn.Linear(dim, n_genes)\n",
    "            else:\n",
    "                self.mean = nn.Sequential(nn.Linear(dim, n_genes), MeanAct())\n",
    "                self.disp = nn.Sequential(nn.Linear(dim, n_genes), DispAct())\n",
    "                self.pi = nn.Sequential(nn.Linear(dim, n_genes), nn.Sigmoid())\n",
    "\n",
    "        \"\"\"Data Augmentation\"\"\"\n",
    "        self.coef=nn.Sequential(\n",
    "                nn.Linear(dim,dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim,1),\n",
    "            )\n",
    "        self.imaug=transforms.Compose([\n",
    "            transforms.RandomGrayscale(0.1),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.RandomHorizontalFlip(0.2),\n",
    "        ])\n",
    "        \"\"\" Regression Module \"\"\"\n",
    "        self.gene_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, n_genes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, patch, adj):\n",
    "        \"\"\" Feature Extraction \"\"\"\n",
    "        h, attn = self.vit(patch, adj)\n",
    "        \n",
    "        \"\"\" Gene expression prediction \"\"\"\n",
    "        x = self.gene_head(h)\n",
    "        \n",
    "        \"\"\"ZINB Distribution\"\"\"\n",
    "        extra=None\n",
    "        if self.zinb>0:\n",
    "            if self.nb:\n",
    "                r=self.hr(h)\n",
    "                p=self.hp(h)\n",
    "                extra=(r,p)\n",
    "            else:\n",
    "                m = self.mean(h)\n",
    "                d = self.disp(h)\n",
    "                p = self.pi(h)\n",
    "                extra=(m,d,p) \n",
    "        return x,extra,h,attn\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        patch, center, exp, adj, oris, sfs, *_ = batch\n",
    "        patch, exp = patch.squeeze(0), exp.squeeze(0)\n",
    "\n",
    "        \"\"\" Model inference \"\"\"\n",
    "        pred,extra,h,attn = self(patch, adj)\n",
    "\n",
    "        \"\"\" Regression Loss \"\"\"\n",
    "        mse_loss = F.mse_loss(pred, exp)\n",
    "        self.log('mse_loss', mse_loss,on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        \"\"\" ZINB Loss \"\"\"\n",
    "        zinb_loss=0\n",
    "        if self.zinb>0:\n",
    "            if self.nb:\n",
    "                r,p=extra\n",
    "                zinb_loss = NB_loss(oris.squeeze(0),r,p)\n",
    "            else:\n",
    "                m,d,p=extra\n",
    "                zinb_loss = ZINB_loss(oris.squeeze(0),m,d,p,sfs.squeeze(0))\n",
    "        self.log('zinb_loss', zinb_loss,on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        \"\"\" Total Loss \"\"\"\n",
    "        loss = mse_loss + self.zinb*zinb_loss\n",
    "        self.log('train_loss', loss,on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        patch, center, exp, adj, oris, sfs, *_ = batch\n",
    "        patch, exp = patch.squeeze(0), exp.squeeze(0)\n",
    "\n",
    "        \"\"\" Model Inference \"\"\"\n",
    "        pred,extra,h,attn = self(patch, adj)\n",
    "\n",
    "        \"\"\" Regression Loss \"\"\"\n",
    "        mse_loss = F.mse_loss(pred, exp)\n",
    "        self.log('val_loss', mse_loss,on_epoch=True, prog_bar=True, logger=True)\n",
    "        return mse_loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        patch, center, exp, adj, oris, sfs, *_ = batch\n",
    "        patch, exp = patch.squeeze(0), exp.squeeze(0)\n",
    "\n",
    "        \"\"\" Model Inference \"\"\"\n",
    "        pred,extra,h,attn = self(patch, adj)\n",
    "\n",
    "        \"\"\"Pearson correlation coeficient\"\"\"\n",
    "        adata1 = ad.AnnData(pred.cpu().detach().numpy())\n",
    "        adata2 = ad.AnnData(exp.cpu().detach().numpy())\n",
    "        R=get_R(adata1,adata2)[0]\n",
    "        mean_pcc=np.nanmean(R)\n",
    "        self.log('test_mean_PCC', mean_pcc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return R\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        optim=torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        optim_dict = {'optimizer': optim}\n",
    "        return optim_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4be1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED TRAINSET\n",
      "Loading imgs...\n",
      "Loading imgs...\n",
      "LOADED TESTSET\n",
      "Test sample CID4290\n",
      "Loading imgs...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For training only\"\"\"\n",
    "import gc\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\"\"\"Training loops\"\"\"\n",
    "seed=12000\n",
    "epochs=1\n",
    "dim=1024\n",
    "name = \"resnet\"   # [\"swin\", \"efficient\", \"resnet\"]\n",
    "fold=1\n",
    "\n",
    "\"\"\"Load dataset\"\"\"\n",
    "train_loader, val_loader = dataset_wrap(fold=fold, train=True, dataloader=True)\n",
    "test_loader = dataset_wrap(fold=fold, train=False, dataloader=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb120aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | vit       | Feature_extractor | 43.5 M\n",
      "1 | mean      | Sequential        | 12.3 K\n",
      "2 | disp      | Sequential        | 12.3 K\n",
      "3 | pi        | Sequential        | 12.3 K\n",
      "4 | coef      | Sequential        | 1.1 M \n",
      "5 | gene_head | Sequential        | 14.3 K\n",
      "------------------------------------------------\n",
      "21.0 M    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "44.6 M    Total params\n",
      "178.213   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 77/77 [00:29<00:00,  2.58it/s, mse_loss_step=0.770, zinb_loss_step=3.690, train_loss_step=1.690]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▏         | 1/45 [00:00<00:29,  1.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▍         | 2/45 [00:00<00:17,  2.48it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|▋         | 3/45 [00:00<00:13,  3.16it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 4/45 [00:01<00:12,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█         | 5/45 [00:01<00:12,  3.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|█▎        | 6/45 [00:02<00:13,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|█▌        | 7/45 [00:02<00:12,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 8/45 [00:02<00:12,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|██        | 9/45 [00:02<00:11,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|██▏       | 10/45 [00:03<00:11,  3.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 11/45 [00:03<00:11,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 12/45 [00:03<00:10,  3.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|██▉       | 13/45 [00:04<00:10,  3.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|███       | 14/45 [00:04<00:09,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 15/45 [00:04<00:08,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▌      | 16/45 [00:04<00:08,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|███▊      | 17/45 [00:04<00:07,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████      | 18/45 [00:05<00:07,  3.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 19/45 [00:05<00:07,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████▍     | 20/45 [00:05<00:07,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████▋     | 21/45 [00:06<00:07,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████▉     | 22/45 [00:06<00:06,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████     | 23/45 [00:06<00:06,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████▎    | 24/45 [00:06<00:06,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████▌    | 25/45 [00:07<00:06,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 26/45 [00:07<00:05,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████    | 27/45 [00:08<00:05,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████▏   | 28/45 [00:08<00:05,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▍   | 29/45 [00:08<00:04,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 30/45 [00:09<00:04,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████▉   | 31/45 [00:09<00:04,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████   | 32/45 [00:09<00:03,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 33/45 [00:10<00:03,  3.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 34/45 [00:10<00:03,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|███████▊  | 35/45 [00:11<00:03,  3.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████  | 36/45 [00:11<00:02,  3.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 37/45 [00:12<00:02,  2.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|████████▍ | 38/45 [00:13<00:02,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|████████▋ | 39/45 [00:13<00:02,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████▉ | 40/45 [00:13<00:01,  2.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 41/45 [00:14<00:01,  2.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|█████████▎| 42/45 [00:14<00:01,  2.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████▌| 43/45 [00:14<00:00,  2.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████▊| 44/45 [00:15<00:00,  2.90it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 77/77 [00:45<00:00,  1.69it/s, mse_loss_step=0.770, zinb_loss_step=3.690, train_loss_step=1.690, val_loss=0.817]\n",
      "Epoch 0: 100%|██████████| 77/77 [00:45<00:00,  1.69it/s, mse_loss_step=0.770, zinb_loss_step=3.690, train_loss_step=1.690, val_loss=0.817, mse_loss_epoch=0.794, zinb_loss_epoch=8.050, train_loss_epoch=2.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 77/77 [00:46<00:00,  1.67it/s, mse_loss_step=0.770, zinb_loss_step=3.690, train_loss_step=1.690, val_loss=0.817, mse_loss_epoch=0.794, zinb_loss_epoch=8.050, train_loss_epoch=2.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:59<00:00, 59.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_mean_PCC       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   -0.07536704603082658    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_mean_PCC      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  -0.07536704603082658   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0.04993143618106842  hours\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Define model\"\"\"\n",
    "model = CNN_GAT(name=name, dim=dim)\n",
    "setup_seed(seed)\n",
    "\n",
    "\"\"\"Setup trainer\"\"\"\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=f\"./{name}_fold{fold}\")\n",
    "trainer = pl.Trainer(accelerator='auto',  callbacks=[EarlyStopping(monitor='val_loss',mode='min')], max_epochs=epochs,logger=False)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "#     torch.save(model.state_dict(),f\"./model/{name}-seed{seed}-epochs{epochs}-sampleIndex{fold}.ckpt\")\n",
    "trainer = pl.Trainer(accelerator='cpu', logger=False)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "\"\"\"Save model and clean memory\"\"\"\n",
    "gc.collect()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"Training time: \", execution_time/3600, \" hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b94e42",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
